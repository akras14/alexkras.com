






































































































































































































































































































































































































































        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:03:34 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
        at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
        at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
        at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:03:34 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
        at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
        at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
        at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:03:34 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
        at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
        at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
        at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:03:34 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
        at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
        at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
        at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:03:34 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:158)
        at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
        at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
        at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:03:34 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:158)
        at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
        at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
        at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
[hadoop@ip-172-31-46-171 ~]$ ^C
[hadoop@ip-172-31-46-171 ~]$ ^C
[hadoop@ip-172-31-46-171 ~]$ vim help-me-god.py

from pyspark import SQLContext, SparkContext, SparkConf
from pyspark.sql import Row
import json
from itertools import izip_longest

if __name__ == '__main__':
    def ziph(num, max):
        return list(izip_longest([], range(num+1, max), fillvalue=num))

    with open("token-tweets.json") as f:
        tt = json.load(f)

    tokent = [set(t) for t in tt]

    conf = SparkConf().setAppName("please work")
    sc = SparkContext(conf=conf)
    #sc = SparkContext()
    sc.setLogLevel("WARN")
    sqlContext = SQLContext(sc)
    rdd = sc.parallelize(tt, numSlices=1000)
    t = rdd.zipWithIndex().filter(lambda x: len(x[0]) > 4).map(lambda x: Row(t1=x[0],i1=x[1]))
    t2 = rdd.zipWithIndex().filter(lambda x: len(x[0]) > 4).map(lambda x: Row(t2=x[0],i2=x[1]))
    df = sqlContext.createDataFrame(t)
    df2 = sqlContext.createDataFrame(t2)

    d = df.crossJoin(df2)

    def findIntersection(r):
        first = tokent[r.i1]
        second = tokent[r.i2]
        intersection = first & second               # Find a sub set of words that is present in both lists
        intersectionLength = float(len(intersection))      # Words both comments have in common
        wordCount = len(first) + len(second)        # Total length of both comments
        score = intersectionLength/wordCount        # Intersection score between two comments
        return Row(t1=r.t1, t2=r.t2, i1=r.i1, i2=r.i2, score=score)

    r = d.filter(d.i1 < d.i2).rdd.map(findIntersection)
    f = sqlContext.createDataFrame(r)
    f.createOrReplaceTempView("TEST")
    ff = sqlContext.sql("select * FROM TEST where score > 0.2")

    with open("t.json", "w") as f:
        for l in ff.toJSON().collect():
            f.write(l + "\n")
~                                                                                                                                                                                  
~                                                                                                                                                                                  
~                                                                                                                                                                                  
~                                                                                                                                                                                  
~                                                                                                                                                                                  
~                                                                                                                                                                                  
~                                                                                                                                                                                  
~                                                                                                                                                                                  
~                                                                                                                                                                                  
"help-me-god.py" 44L, 1711C written
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$ spark-submit help-me-god.py
18/09/14 16:03:51 INFO SparkContext: Running Spark version 2.3.1
18/09/14 16:03:51 INFO SparkContext: Submitted application: please work
18/09/14 16:03:51 INFO SecurityManager: Changing view acls to: hadoop
18/09/14 16:03:51 INFO SecurityManager: Changing modify acls to: hadoop
18/09/14 16:03:51 INFO SecurityManager: Changing view acls groups to:
18/09/14 16:03:51 INFO SecurityManager: Changing modify acls groups to:
18/09/14 16:03:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
18/09/14 16:03:51 INFO Utils: Successfully started service 'sparkDriver' on port 38875.
18/09/14 16:03:51 INFO SparkEnv: Registering MapOutputTracker
18/09/14 16:03:51 INFO SparkEnv: Registering BlockManagerMaster
18/09/14 16:03:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/09/14 16:03:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/09/14 16:03:51 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-8311ba8a-42ba-4916-af68-b56376a409c3
18/09/14 16:03:51 INFO MemoryStore: MemoryStore started with capacity 5.9 GB
18/09/14 16:03:51 INFO SparkEnv: Registering OutputCommitCoordinator
18/09/14 16:03:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/09/14 16:03:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-46-171.us-west-2.compute.internal:4040
18/09/14 16:03:52 INFO Utils: Using initial executors = 9, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
18/09/14 16:03:53 INFO RMProxy: Connecting to ResourceManager at ip-172-31-46-171.us-west-2.compute.internal/172.31.46.171:8032
18/09/14 16:03:53 INFO Client: Requesting a new application from cluster with 9 NodeManagers
18/09/14 16:03:53 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)
18/09/14 16:03:53 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
18/09/14 16:03:53 INFO Client: Setting up container launch context for our AM
18/09/14 16:03:53 INFO Client: Setting up the launch environment for our AM container
18/09/14 16:03:54 INFO Client: Preparing resources for our AM container
18/09/14 16:03:55 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
18/09/14 16:03:58 INFO Client: Uploading resource file:/mnt/tmp/spark-a6e165d9-1533-4152-b52f-deac113fedbb/__spark_libs__1425131409458612070.zip -> hdfs://ip-172-31-46-171.us-west-2.compute.internal:8020/user/hadoop/.sparkStaging/application_1536937031130_0004/__spark_libs__1425131409458612070.zip
18/09/14 16:03:58 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException
java.util.NoSuchElementException
        at java.util.Collections$EmptyIterator.next(Collections.java:4189)
        at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
        at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
        at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
        at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
        at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
        at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
        at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
        at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
        at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
        at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
        at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.spark_project.jetty.server.Server.handle(Server.java:534)
        at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:03:58 WARN ServletHandler: /jobs/
java.util.NoSuchElementException
        at java.util.Collections$EmptyIterator.next(Collections.java:4189)
        at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
        at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
        at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
        at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
        at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
        at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
        at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
        at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
        at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
        at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
        at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.spark_project.jetty.server.Server.handle(Server.java:534)
        at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:03:59 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-46-171.us-west-2.compute.internal:8020/user/hadoop/.sparkStaging/application_1536937031130_0004/pyspark.zip
18/09/14 16:03:59 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-31-46-171.us-west-2.compute.internal:8020/user/hadoop/.sparkStaging/application_1536937031130_0004/py4j-0.10.7-src.zip
18/09/14 16:03:59 INFO Client: Uploading resource file:/mnt/tmp/spark-a6e165d9-1533-4152-b52f-deac113fedbb/__spark_conf__6298753532563516708.zip -> hdfs://ip-172-31-46-171.us-west-2.compute.internal:8020/user/hadoop/.sparkStaging/application_1536937031130_0004/__spark_conf__.zip
18/09/14 16:03:59 INFO SecurityManager: Changing view acls to: hadoop
18/09/14 16:03:59 INFO SecurityManager: Changing modify acls to: hadoop
18/09/14 16:03:59 INFO SecurityManager: Changing view acls groups to:
18/09/14 16:03:59 INFO SecurityManager: Changing modify acls groups to:
18/09/14 16:03:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
18/09/14 16:03:59 INFO Client: Submitting application application_1536937031130_0004 to ResourceManager
18/09/14 16:03:59 INFO YarnClientImpl: Submitted application application_1536937031130_0004
18/09/14 16:03:59 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1536937031130_0004 and attemptId None
18/09/14 16:04:00 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException
java.util.NoSuchElementException
        at java.util.Collections$EmptyIterator.next(Collections.java:4189)
        at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
        at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
        at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
        at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
        at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
        at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
        at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
        at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
        at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
        at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
        at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.spark_project.jetty.server.Server.handle(Server.java:534)
        at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:04:00 WARN ServletHandler: /jobs/
java.util.NoSuchElementException
        at java.util.Collections$EmptyIterator.next(Collections.java:4189)
        at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
        at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
        at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
        at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
        at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
        at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
        at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
        at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
        at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
        at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
        at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.spark_project.jetty.server.Server.handle(Server.java:534)
        at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
18/09/14 16:04:00 INFO Client: Application report for application_1536937031130_0004 (state: ACCEPTED)
18/09/14 16:04:00 INFO Client:
         client token: N/A
         diagnostics: AM container is launched, waiting for AM container to Register with RM
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: default
         start time: 1536941039495
         final status: UNDEFINED
         tracking URL: http://ip-172-31-46-171.us-west-2.compute.internal:20888/proxy/application_1536937031130_0004/
         user: hadoop
18/09/14 16:04:01 INFO Client: Application report for application_1536937031130_0004 (state: ACCEPTED)
18/09/14 16:04:02 INFO Client: Application report for application_1536937031130_0004 (state: ACCEPTED)
18/09/14 16:04:03 INFO Client: Application report for application_1536937031130_0004 (state: ACCEPTED)
18/09/14 16:04:04 INFO Client: Application report for application_1536937031130_0004 (state: ACCEPTED)
18/09/14 16:04:04 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-46-171.us-west-2.compute.internal, PROXY_URI_BASES -> http://ip-172-31-46-171.us-west-2.compute.internal:20888/proxy/application_1536937031130_0004), /proxy/application_1536937031130_0004
18/09/14 16:04:04 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
18/09/14 16:04:05 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
18/09/14 16:04:05 INFO Client: Application report for application_1536937031130_0004 (state: RUNNING)
18/09/14 16:04:05 INFO Client:
         client token: N/A
         diagnostics: N/A
         ApplicationMaster host: 172.31.36.18
         ApplicationMaster RPC port: 0
         queue: default
         start time: 1536941039495
         final status: UNDEFINED
         tracking URL: http://ip-172-31-46-171.us-west-2.compute.internal:20888/proxy/application_1536937031130_0004/
         user: hadoop
18/09/14 16:04:05 INFO YarnClientSchedulerBackend: Application application_1536937031130_0004 has started running.
18/09/14 16:04:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43859.
18/09/14 16:04:05 INFO NettyBlockTransferService: Server created on ip-172-31-46-171.us-west-2.compute.internal:43859
18/09/14 16:04:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/09/14 16:04:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-46-171.us-west-2.compute.internal, 43859, None)
18/09/14 16:04:05 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-46-171.us-west-2.compute.internal:43859 with 5.9 GB RAM, BlockManagerId(driver, ip-172-31-46-171.us-west-2.compute.internal, 43859, None)
18/09/14 16:04:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-46-171.us-west-2.compute.internal, 43859, None)
18/09/14 16:04:05 INFO BlockManager: external shuffle service port = 7337
18/09/14 16:04:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-46-171.us-west-2.compute.internal, 43859, None)
18/09/14 16:04:06 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1536937031130_0004
18/09/14 16:04:06 INFO Utils: Using initial executors = 9, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
18/09/14 16:04:09 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.36.18:46994) with ID 5
18/09/14 16:04:09 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 1)
18/09/14 16:04:09 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-36-18.us-west-2.compute.internal:43697 with 5.5 GB RAM, BlockManagerId(5, ip-172-31-36-18.us-west-2.compute.internal, 43697, None)
18/09/14 16:04:09 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.41.220:42004) with ID 1
18/09/14 16:04:09 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 2)
18/09/14 16:04:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-41-220.us-west-2.compute.internal:41415 with 5.5 GB RAM, BlockManagerId(1, ip-172-31-41-220.us-west-2.compute.internal, 41415, None)
18/09/14 16:04:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.33.251:57936) with ID 2
18/09/14 16:04:10 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 3)
18/09/14 16:04:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.46.99:48196) with ID 3
18/09/14 16:04:10 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 4)
18/09/14 16:04:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-33-251.us-west-2.compute.internal:42549 with 5.5 GB RAM, BlockManagerId(2, ip-172-31-33-251.us-west-2.compute.internal, 42549, None)
18/09/14 16:04:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-46-99.us-west-2.compute.internal:40091 with 5.5 GB RAM, BlockManagerId(3, ip-172-31-46-99.us-west-2.compute.internal, 40091, None)
18/09/14 16:04:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.44.36:52944) with ID 4
18/09/14 16:04:10 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 5)
18/09/14 16:04:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-44-36.us-west-2.compute.internal:39733 with 5.5 GB RAM, BlockManagerId(4, ip-172-31-44-36.us-west-2.compute.internal, 39733, None)
18/09/14 16:04:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.40.48:59916) with ID 8
18/09/14 16:04:10 INFO ExecutorAllocationManager: New executor 8 has registered (new total is 6)
18/09/14 16:04:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.32.177:39698) with ID 6
18/09/14 16:04:10 INFO ExecutorAllocationManager: New executor 6 has registered (new total is 7)
18/09/14 16:04:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.32.246:41114) with ID 7
18/09/14 16:04:10 INFO ExecutorAllocationManager: New executor 7 has registered (new total is 8)
18/09/14 16:04:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-40-48.us-west-2.compute.internal:34719 with 5.5 GB RAM, BlockManagerId(8, ip-172-31-40-48.us-west-2.compute.internal, 34719, None)
18/09/14 16:04:10 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8









18/09/15 07:48:26 ERROR TaskSetManager: Total size of serialized results of 470224 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)
18/09/15 07:48:26 WARN TaskSetManager: Lost task 470292.0 in stage 5.0 (TID 472295, ip-172-31-41-220.us-west-2.compute.internal, executor 11): TaskKilled (Stage cancelled)
18/09/15 07:48:26 WARN TaskSetManager: Lost task 470294.0 in stage 5.0 (TID 472297, ip-172-31-46-99.us-west-2.compute.internal, executor 14): TaskKilled (Stage cancelled)
Traceback (most recent call last):
  File "/home/hadoop/help-me-god.py", line 43, in <module>
    for l in ff.toJSON().collect():
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 834, in collect
  File "/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 470224 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1753)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1741)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1740)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1740)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1974)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1923)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1912)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:938)
        at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)
        at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)

18/09/15 07:48:26 WARN TaskSetManager: Lost task 470295.0 in stage 5.0 (TID 472298, ip-172-31-40-48.us-west-2.compute.internal, executor 8): TaskKilled (Stage cancelled)
18/09/15 07:48:26 WARN TaskSetManager: Lost task 470219.0 in stage 5.0 (TID 472222, ip-172-31-32-246.us-west-2.compute.internal, executor 16): TaskKilled (Stage cancelled)
18/09/15 07:48:26 WARN TaskSetManager: Lost task 470227.0 in stage 5.0 (TID 472230, ip-172-31-46-99.us-west-2.compute.internal, executor 14): TaskKilled (Stage cancelled)
18/09/15 07:48:26 WARN TaskSetManager: Lost task 470228.0 in stage 5.0 (TID 472231, ip-172-31-40-48.us-west-2.compute.internal, executor 8): TaskKilled (Stage cancelled)
18/09/15 07:48:26 WARN TaskSetManager: Lost task 470230.0 in stage 5.0 (TID 472233, ip-172-31-41-220.us-west-2.compute.internal, executor 11): TaskKilled (Stage cancelled)
18/09/15 07:48:26 WARN TaskSetManager: Lost task 470231.0 in stage 5.0 (TID 472234, ip-172-31-40-48.us-west-2.compute.internal, executor 8): TaskKilled (Stage cancelled)
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
[hadoop@ip-172-31-46-171 ~]$
